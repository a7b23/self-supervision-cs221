Logging to /tmp/openai-2019-10-24-19-28-41-657434
/afs/cs.stanford.edu/u/prabhat8/rl_env/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/afs/cs.stanford.edu/u/prabhat8/rl_env/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/afs/cs.stanford.edu/u/prabhat8/rl_env/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/afs/cs.stanford.edu/u/prabhat8/rl_env/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/afs/cs.stanford.edu/u/prabhat8/rl_env/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/afs/cs.stanford.edu/u/prabhat8/rl_env/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/afs/cs.stanford.edu/u/prabhat8/rl_env/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/afs/cs.stanford.edu/u/prabhat8/rl_env/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/afs/cs.stanford.edu/u/prabhat8/rl_env/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/afs/cs.stanford.edu/u/prabhat8/rl_env/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/afs/cs.stanford.edu/u/prabhat8/rl_env/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/afs/cs.stanford.edu/u/prabhat8/rl_env/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
WARNING:tensorflow:From /afs/cs.stanford.edu/u/prabhat8/baselines/baselines/common/tf_util.py:53: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.

WARNING:tensorflow:From /afs/cs.stanford.edu/u/prabhat8/baselines/baselines/common/tf_util.py:63: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

WARNING:tensorflow:From /afs/cs.stanford.edu/u/prabhat8/baselines/baselines/common/tf_util.py:70: The name tf.InteractiveSession is deprecated. Please use tf.compat.v1.InteractiveSession instead.

2019-10-24 19:28:42.766163: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-10-24 19:28:42.786053: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
2019-10-24 19:28:43.099039: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5690360 executing computations on platform CUDA. Devices:
2019-10-24 19:28:43.099156: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): TITAN X (Pascal), Compute Capability 6.1
2019-10-24 19:28:43.104557: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2593895000 Hz
2019-10-24 19:28:43.105863: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5706100 executing computations on platform Host. Devices:
2019-10-24 19:28:43.105939: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2019-10-24 19:28:43.107026: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: TITAN X (Pascal) major: 6 minor: 1 memoryClockRate(GHz): 1.531
pciBusID: 0000:03:00.0
2019-10-24 19:28:43.107237: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcudart.so.10.0'; dlerror: libcudart.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:
2019-10-24 19:28:43.107338: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcublas.so.10.0'; dlerror: libcublas.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:
2019-10-24 19:28:43.107433: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcufft.so.10.0'; dlerror: libcufft.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:
2019-10-24 19:28:43.107527: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcurand.so.10.0'; dlerror: libcurand.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:
2019-10-24 19:28:43.107619: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcusolver.so.10.0'; dlerror: libcusolver.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:
2019-10-24 19:28:43.107710: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcusparse.so.10.0'; dlerror: libcusparse.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:
2019-10-24 19:28:43.110536: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-10-24 19:28:43.110568: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1663] Cannot dlopen some GPU libraries. Skipping registering GPU devices...
2019-10-24 19:28:43.110591: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-10-24 19:28:43.110604: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2019-10-24 19:28:43.110614: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
WARNING:tensorflow:From /afs/cs.stanford.edu/u/prabhat8/baselines/baselines/common/misc_util.py:58: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.

WARNING:tensorflow:From /afs/cs.stanford.edu/u/prabhat8/baselines/baselines/deepq/deepq.py:205: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.

WARNING:tensorflow:From /afs/cs.stanford.edu/u/prabhat8/baselines/baselines/deepq/build_graph.py:176: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

WARNING:tensorflow:From /afs/cs.stanford.edu/u/prabhat8/baselines/baselines/common/input.py:57: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7fe0d51d47b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7fe0d51d47b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7fe0d51d4748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7fe0d51d4748>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7fe0d51d4668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7fe0d51d4668>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:From /afs/cs.stanford.edu/u/prabhat8/rl_env/lib/python3.5/site-packages/tensorflow/contrib/layers/python/layers/layers.py:1634: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.flatten instead.
WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7fe0d51d4358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7fe0d51d4358>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe0d51d4550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe0d51d4550>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe1775b1ef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe1775b1ef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe0d51d4550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe0d51d4550>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe1775b1ef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe1775b1ef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:From /afs/cs.stanford.edu/u/prabhat8/baselines/baselines/deepq/build_graph.py:189: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7fe0cc4afb00>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7fe0cc4afb00>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7fe0cc4af8d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7fe0cc4af8d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7fe0cc4afe80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7fe0cc4afe80>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7fe0cc4af748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7fe0cc4af748>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe0cc4af978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe0cc4af978>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe0cc4afcf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe0cc4afcf8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe0cc4af6d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe0cc4af6d8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe0cc4af6d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe0cc4af6d8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7fe0cc4cc3c8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7fe0cc4cc3c8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7fe0cc4cc128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7fe0cc4cc128>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7fe0cc4cc048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7fe0cc4cc048>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7fe0d401a780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7fe0d401a780>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe0cc4af400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe0cc4af400>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe0cc4af400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe0cc4af400>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe0cc4b8390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe0cc4b8390>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe0cc4b8390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe0cc4b8390>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7fe0cc456898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7fe0cc456898>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7fe0cc4566a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7fe0cc4566a0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7fe0cc456b70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7fe0cc456b70>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7fe0cc43d438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7fe0cc43d438>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe0cc456d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe0cc456d68>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe0cc456d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe0cc456d68>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe0cc456d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe0cc456d68>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe0cc456d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe0cc456d68>>: AssertionError: Bad argument number for Name: 3, expecting 4
2019-10-24 19:28:47.014958: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
env_type: atari
Training deepq on atari:PongNoFrameskip-v4 with arguments 
{'target_network_update_freq': 1000, 'checkpoint_path': None, 'lr': 0.0001, 'prioritized_replay_alpha': 0.6, 'exploration_final_eps': 0.01, 'gamma': 0.99, 'learning_starts': 10000, 'buffer_size': 10000, 'network': 'conv_only', 'train_freq': 4, 'checkpoint_freq': 10000, 'prioritized_replay': True, 'exploration_fraction': 0.1, 'dueling': True}
--------------------------------------
| % time spent exploring  | 98       |
| episodes                | 2        |
| mean 100 episode reward | -18      |
| steps                   | 1.11e+03 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 97       |
| episodes                | 3        |
| mean 100 episode reward | -19.5    |
| steps                   | 2.05e+03 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 97       |
| episodes                | 4        |
| mean 100 episode reward | -20      |
| steps                   | 2.9e+03  |
--------------------------------------
--------------------------------------
| % time spent exploring  | 96       |
| episodes                | 5        |
| mean 100 episode reward | -20      |
| steps                   | 3.85e+03 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 95       |
| episodes                | 6        |
| mean 100 episode reward | -20      |
| steps                   | 4.84e+03 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 94       |
| episodes                | 7        |
| mean 100 episode reward | -20      |
| steps                   | 5.72e+03 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 93       |
| episodes                | 8        |
| mean 100 episode reward | -20.1    |
| steps                   | 6.56e+03 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 92       |
| episodes                | 9        |
| mean 100 episode reward | -20.1    |
| steps                   | 7.56e+03 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 91       |
| episodes                | 10       |
| mean 100 episode reward | -20.1    |
| steps                   | 8.52e+03 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 90       |
| episodes                | 11       |
| mean 100 episode reward | -20.1    |
| steps                   | 9.42e+03 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 89       |
| episodes                | 12       |
| mean 100 episode reward | -20.2    |
| steps                   | 1.02e+04 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 89       |
| episodes                | 13       |
| mean 100 episode reward | -20.2    |
| steps                   | 1.11e+04 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 88       |
| episodes                | 14       |
| mean 100 episode reward | -20.2    |
| steps                   | 1.2e+04  |
--------------------------------------
--------------------------------------
| % time spent exploring  | 87       |
| episodes                | 15       |
| mean 100 episode reward | -20.3    |
| steps                   | 1.3e+04  |
--------------------------------------
--------------------------------------
| % time spent exploring  | 86       |
| episodes                | 16       |
| mean 100 episode reward | -20.3    |
| steps                   | 1.41e+04 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 85       |
| episodes                | 17       |
| mean 100 episode reward | -20.2    |
| steps                   | 1.5e+04  |
--------------------------------------
--------------------------------------
| % time spent exploring  | 84       |
| episodes                | 18       |
| mean 100 episode reward | -20.2    |
| steps                   | 1.6e+04  |
--------------------------------------
--------------------------------------
| % time spent exploring  | 83       |
| episodes                | 19       |
| mean 100 episode reward | -20.2    |
| steps                   | 1.69e+04 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 82       |
| episodes                | 20       |
| mean 100 episode reward | -20.1    |
| steps                   | 1.8e+04  |
--------------------------------------
--------------------------------------
| % time spent exploring  | 81       |
| episodes                | 21       |
| mean 100 episode reward | -20.1    |
| steps                   | 1.9e+04  |
--------------------------------------
--------------------------------------
| % time spent exploring  | 80       |
| episodes                | 22       |
| mean 100 episode reward | -20.1    |
| steps                   | 1.99e+04 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 79       |
| episodes                | 23       |
| mean 100 episode reward | -20      |
| steps                   | 2.1e+04  |
--------------------------------------
--------------------------------------
| % time spent exploring  | 78       |
| episodes                | 24       |
| mean 100 episode reward | -20      |
| steps                   | 2.2e+04  |
--------------------------------------
--------------------------------------
| % time spent exploring  | 77       |
| episodes                | 25       |
| mean 100 episode reward | -20      |
| steps                   | 2.3e+04  |
--------------------------------------
--------------------------------------
| % time spent exploring  | 76       |
| episodes                | 26       |
| mean 100 episode reward | -20      |
| steps                   | 2.39e+04 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 75       |
| episodes                | 27       |
| mean 100 episode reward | -20.1    |
| steps                   | 2.47e+04 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 74       |
| episodes                | 28       |
| mean 100 episode reward | -20.1    |
| steps                   | 2.58e+04 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 73       |
| episodes                | 29       |
| mean 100 episode reward | -20.1    |
| steps                   | 2.66e+04 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 72       |
| episodes                | 30       |
| mean 100 episode reward | -20.1    |
| steps                   | 2.76e+04 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 71       |
| episodes                | 31       |
| mean 100 episode reward | -20.1    |
| steps                   | 2.86e+04 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 70       |
| episodes                | 32       |
| mean 100 episode reward | -20.1    |
| steps                   | 2.94e+04 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 70       |
| episodes                | 33       |
| mean 100 episode reward | -20.2    |
| steps                   | 3.02e+04 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 69       |
| episodes                | 34       |
| mean 100 episode reward | -20.2    |
| steps                   | 3.11e+04 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 68       |
| episodes                | 35       |
| mean 100 episode reward | -20.2    |
| steps                   | 3.2e+04  |
--------------------------------------
--------------------------------------
| % time spent exploring  | 67       |
| episodes                | 36       |
| mean 100 episode reward | -20.2    |
| steps                   | 3.28e+04 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 66       |
| episodes                | 37       |
| mean 100 episode reward | -20.1    |
| steps                   | 3.39e+04 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 65       |
| episodes                | 38       |
| mean 100 episode reward | -20.2    |
| steps                   | 3.48e+04 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 64       |
| episodes                | 39       |
| mean 100 episode reward | -20.2    |
| steps                   | 3.57e+04 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 63       |
| episodes                | 40       |
| mean 100 episode reward | -20.2    |
| steps                   | 3.67e+04 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 62       |
| episodes                | 41       |
| mean 100 episode reward | -20.2    |
| steps                   | 3.75e+04 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 62       |
| episodes                | 42       |
| mean 100 episode reward | -20.2    |
| steps                   | 3.83e+04 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 61       |
| episodes                | 43       |
| mean 100 episode reward | -20.2    |
| steps                   | 3.92e+04 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 60       |
| episodes                | 44       |
| mean 100 episode reward | -20.2    |
| steps                   | 4.01e+04 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 59       |
| episodes                | 45       |
| mean 100 episode reward | -20.2    |
| steps                   | 4.09e+04 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 58       |
| episodes                | 46       |
| mean 100 episode reward | -20.2    |
| steps                   | 4.18e+04 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 57       |
| episodes                | 47       |
| mean 100 episode reward | -20.3    |
| steps                   | 4.27e+04 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 56       |
| episodes                | 48       |
| mean 100 episode reward | -20.3    |
| steps                   | 4.35e+04 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 56       |
| episodes                | 49       |
| mean 100 episode reward | -20.3    |
| steps                   | 4.43e+04 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 55       |
| episodes                | 50       |
| mean 100 episode reward | -20.3    |
| steps                   | 4.52e+04 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 54       |
| episodes                | 51       |
| mean 100 episode reward | -20.3    |
| steps                   | 4.62e+04 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 53       |
| episodes                | 52       |
| mean 100 episode reward | -20.3    |
| steps                   | 4.72e+04 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 52       |
| episodes                | 53       |
| mean 100 episode reward | -20.3    |
| steps                   | 4.79e+04 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 51       |
| episodes                | 54       |
| mean 100 episode reward | -20.3    |
| steps                   | 4.88e+04 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 50       |
| episodes                | 55       |
| mean 100 episode reward | -20.3    |
| steps                   | 4.96e+04 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 50       |
| episodes                | 56       |
| mean 100 episode reward | -20.3    |
| steps                   | 5.04e+04 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 49       |
| episodes                | 57       |
| mean 100 episode reward | -20.3    |
| steps                   | 5.12e+04 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 48       |
| episodes                | 58       |
| mean 100 episode reward | -20.3    |
| steps                   | 5.23e+04 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 47       |
| episodes                | 59       |
| mean 100 episode reward | -20.3    |
| steps                   | 5.31e+04 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 46       |
| episodes                | 60       |
| mean 100 episode reward | -20.3    |
| steps                   | 5.4e+04  |
--------------------------------------
--------------------------------------
| % time spent exploring  | 45       |
| episodes                | 61       |
| mean 100 episode reward | -20.4    |
| steps                   | 5.49e+04 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 44       |
| episodes                | 62       |
| mean 100 episode reward | -20.4    |
| steps                   | 5.58e+04 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 43       |
| episodes                | 63       |
| mean 100 episode reward | -20.4    |
| steps                   | 5.66e+04 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 43       |
| episodes                | 64       |
| mean 100 episode reward | -20.4    |
| steps                   | 5.74e+04 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 42       |
| episodes                | 65       |
| mean 100 episode reward | -20.4    |
| steps                   | 5.83e+04 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 41       |
| episodes                | 66       |
| mean 100 episode reward | -20.3    |
| steps                   | 5.95e+04 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 40       |
| episodes                | 67       |
| mean 100 episode reward | -20.3    |
| steps                   | 6.06e+04 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 39       |
| episodes                | 68       |
| mean 100 episode reward | -20.3    |
| steps                   | 6.14e+04 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 38       |
| episodes                | 69       |
| mean 100 episode reward | -20.4    |
| steps                   | 6.23e+04 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 37       |
| episodes                | 70       |
| mean 100 episode reward | -20.4    |
| steps                   | 6.31e+04 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 36       |
| episodes                | 71       |
| mean 100 episode reward | -20.4    |
| steps                   | 6.4e+04  |
--------------------------------------
--------------------------------------
| % time spent exploring  | 35       |
| episodes                | 72       |
| mean 100 episode reward | -20.4    |
| steps                   | 6.49e+04 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 34       |
| episodes                | 73       |
| mean 100 episode reward | -20.4    |
| steps                   | 6.58e+04 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 33       |
| episodes                | 74       |
| mean 100 episode reward | -20.4    |
| steps                   | 6.69e+04 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 32       |
| episodes                | 75       |
| mean 100 episode reward | -20.4    |
| steps                   | 6.78e+04 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 31       |
| episodes                | 76       |
| mean 100 episode reward | -20.3    |
| steps                   | 6.88e+04 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 30       |
| episodes                | 77       |
| mean 100 episode reward | -20.3    |
| steps                   | 7e+04    |
--------------------------------------
--------------------------------------
| % time spent exploring  | 29       |
| episodes                | 78       |
| mean 100 episode reward | -20.3    |
| steps                   | 7.11e+04 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 28       |
| episodes                | 79       |
| mean 100 episode reward | -20.3    |
| steps                   | 7.21e+04 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 27       |
| episodes                | 80       |
| mean 100 episode reward | -20.3    |
| steps                   | 7.31e+04 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 26       |
| episodes                | 81       |
| mean 100 episode reward | -20.3    |
| steps                   | 7.42e+04 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 25       |
| episodes                | 82       |
| mean 100 episode reward | -20.3    |
| steps                   | 7.52e+04 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 24       |
| episodes                | 83       |
| mean 100 episode reward | -20.3    |
| steps                   | 7.62e+04 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 23       |
| episodes                | 84       |
| mean 100 episode reward | -20.3    |
| steps                   | 7.7e+04  |
--------------------------------------
--------------------------------------
| % time spent exploring  | 22       |
| episodes                | 85       |
| mean 100 episode reward | -20.3    |
| steps                   | 7.8e+04  |
--------------------------------------
--------------------------------------
| % time spent exploring  | 21       |
| episodes                | 86       |
| mean 100 episode reward | -20.3    |
| steps                   | 7.93e+04 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 20       |
| episodes                | 87       |
| mean 100 episode reward | -20.3    |
| steps                   | 8.01e+04 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 19       |
| episodes                | 88       |
| mean 100 episode reward | -20.3    |
| steps                   | 8.1e+04  |
--------------------------------------
--------------------------------------
| % time spent exploring  | 18       |
| episodes                | 89       |
| mean 100 episode reward | -20.3    |
| steps                   | 8.19e+04 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 18       |
| episodes                | 90       |
| mean 100 episode reward | -20.3    |
| steps                   | 8.28e+04 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 17       |
| episodes                | 91       |
| mean 100 episode reward | -20.3    |
| steps                   | 8.38e+04 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 16       |
| episodes                | 92       |
| mean 100 episode reward | -20.3    |
| steps                   | 8.46e+04 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 15       |
| episodes                | 93       |
| mean 100 episode reward | -20.3    |
| steps                   | 8.55e+04 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 14       |
| episodes                | 94       |
| mean 100 episode reward | -20.3    |
| steps                   | 8.65e+04 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 13       |
| episodes                | 95       |
| mean 100 episode reward | -20.3    |
| steps                   | 8.77e+04 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 12       |
| episodes                | 96       |
| mean 100 episode reward | -20.3    |
| steps                   | 8.87e+04 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 11       |
| episodes                | 97       |
| mean 100 episode reward | -20.3    |
| steps                   | 8.99e+04 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 9        |
| episodes                | 98       |
| mean 100 episode reward | -20.3    |
| steps                   | 9.09e+04 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 8        |
| episodes                | 99       |
| mean 100 episode reward | -20.3    |
| steps                   | 9.2e+04  |
--------------------------------------
--------------------------------------
| % time spent exploring  | 7        |
| episodes                | 100      |
| mean 100 episode reward | -20.3    |
| steps                   | 9.31e+04 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 6        |
| episodes                | 101      |
| mean 100 episode reward | -20.2    |
| steps                   | 9.43e+04 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 5        |
| episodes                | 102      |
| mean 100 episode reward | -20.2    |
| steps                   | 9.54e+04 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 4        |
| episodes                | 103      |
| mean 100 episode reward | -20.2    |
| steps                   | 9.65e+04 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 3        |
| episodes                | 104      |
| mean 100 episode reward | -20.2    |
| steps                   | 9.77e+04 |
--------------------------------------
--------------------------------------
| % time spent exploring  | 1        |
| episodes                | 105      |
| mean 100 episode reward | -20.2    |
| steps                   | 9.9e+04  |
--------------------------------------
--------------------------------------
| % time spent exploring  | 1        |
| episodes                | 106      |
| mean 100 episode reward | -20.2    |
| steps                   | 1e+05    |
--------------------------------------
Saving model due to mean reward increase: None -> -20.2
/afs/cs.stanford.edu/u/prabhat8/rl_env/lib/python3.5/site-packages/numpy/core/fromnumeric.py:3257: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/afs/cs.stanford.edu/u/prabhat8/rl_env/lib/python3.5/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
Traceback (most recent call last):
  File "/usr/lib/python3.5/runpy.py", line 184, in _run_module_as_main
    "__main__", mod_spec)
  File "/usr/lib/python3.5/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/afs/cs.stanford.edu/u/prabhat8/baselines/baselines/run.py", line 250, in <module>
    main(sys.argv)
  File "/afs/cs.stanford.edu/u/prabhat8/baselines/baselines/run.py", line 216, in main
    model, env = train(args, extra_args)
  File "/afs/cs.stanford.edu/u/prabhat8/baselines/baselines/run.py", line 80, in train
    **alg_kwargs
  File "/afs/cs.stanford.edu/u/prabhat8/baselines/baselines/deepq/deepq.py", line 324, in learn
    save_variables(model_file)
  File "/afs/cs.stanford.edu/u/prabhat8/baselines/baselines/common/tf_util.py", line 346, in save_variables
    import joblib
ImportError: No module named 'joblib'
